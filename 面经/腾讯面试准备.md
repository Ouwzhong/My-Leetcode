# 面试准备

[toc]

### 自我介绍
...

### 项目问题准备

### 高校舆情项目

1. 意见领袖算法：根据帖子id排序选出热度前三。将数据读入计算热度，然后用堆排序的方法取出热度前三的。后期可以升级为添加一个热度的字段，然后在这个热度字段上建立索引，直接食用sql优化排序。
2. 热度算法，目前是用转发+评论数，但这个衡量方法存在差异，比如斗鱼的热度并不代表真实人数。以后希望用加权的方法，找到一个平衡点，平衡微博微信等等不同帖子的热度计算方法。
3. 舆情案例推荐算法，jaccard相似度，用风险点匹配的方法
4. 相似度算法 TFIDF

#### 难点和解决方案

本项目本质上还是一个算法的服务器，而不是一个web后台服务器，因此遇到的技术难点大多都在算法的设计这方面。其实算法的复杂性都比较低，但可能因为数据量有一些大，所以有时候就会出现一些延迟问题，需要去优化。

1. 在相似度匹配算法中我用了TFIDF的计算方法去计算，这个算法需要对帖子的词语收集统计数量，然后得到一个TFIDF的矩阵来计算相似性。不过呢因为数据量比较大， 所以在多个用户同时请求接口去计算TFIDF的会有一些延迟，然后经过思考，虽然数据是一直在更新的，不过短期内涌入的数据量还是比较少的，因此没必要每次都重新去计算TFIDF矩阵，所以就加了一个**flask-cahce**缓存组建，在一定时间内都返回同样的数据，避免了反复计算。然后设置时间可以定时重新计算。
2. 然后每个他是会有一个热度值的，然后经常会涉及到**热度的排序或者取前几个展示**。但是因为现阶段还没有研究出热度计算的算法，都是用转发量+评论量来代替的，因此每次都要读入转发量和评论量计算然后我这边用堆排序的方法获取结果，这样在数据量很大的时候，也会出现一些延迟问题，并且需要全部读入数据才能排序。后面呢，我就建议增加一个单独的热度字段，然后在上面直接建立B+树的索引，这样就可以直接用sql去查询，而且效率有很大提升了。
3. 还有一个比较难的就是**算法的设计问题**了，他们是一个舆情的监控系统，希望用户可以创建了一个话题事件后，根据和这个时间相关的帖子去给出一个风险值，然后希望能融入一些他们传播学的理论和机制。传播学对这种一般会分成：敏感性、真实性和传播性三种。经过讨论后我们就设计了一个类似击中击不中的算法，让他们把这三个属性再细分，用 01标签打好，用来代表一个话题事件的属性。



#### 分布式文件系统

##### 副本同步方法

1. 先说知道的三种普遍的同步模式：主从同步，采用日志的方式，分为链式和扇出，还有客户端同步的方式。
2. 讲一下自己的实现方式是主从模式中的扇出的方法，解决的问题是会出现循环发送的问题，因此加多一个字段表明这个操作是否还需要进一步扩散同步。
3. 提出一个新的传染病模型的同步方式，主服务器先将数据同步给几个副本服务器，再由这几个副本服务器去扩散信息

##### 同步异步问题

1. 先说同步和异步分别是什么。同步延迟大，异步数据同步不及时，为什么都会影响用户体验

2. 我项目这边因为是个小项目，也没有很多实际的使用场景，因此这边选择的是同步的方案。

3. 真实使用的时候要看场景决定，同步的话保证主副本一直一样，如果出现数据丢失或者Leader失效可以恢复，但如果出现网络或节点的故障，会导致无法处理写入，导致系统缓慢。

   异步一般就性能好，用户体验好，举出onedrive或高校舆情的例子。

4. 实际中使用的话一般会采用半同步的机制，就是如果有两个副本服务器，一个采用同步一个采用异步，可以兼具两者的优点。

##### Learder选取

1. 主从服务器的方法其实是类似一种raft协议，raft协议中隔段时间回去选取新的Leader
2. 但在项目中因为采取的是同步方案，所以所有的副本都是一样的数据，每个副本都可以做Leader，所以就没有写。其实是相当于一个用户选取主服务器的方法，用户选择的服务器，就是主服务器

##### 锁机制

1. 在用户下载、删除、打开文件的时候要向锁服务器申请加文件锁，如果文件加锁失败，会输出无法操作。

##### 打开和关闭文件

1. 打开文件是模拟一个在线编辑的过程，但要实现一个在线查看的软件比较麻烦，所以只是一个大概的模拟。调用打开文件时，先给目标文件上文件锁，将目标文件下载到本地，然后用户需要自己去用电脑的软件打开文件并编辑文件，然后调用指令查看当前已打开文件，然后查看对应的文件id，使用close指令关闭对应打开的文件。系统也会把修改同步上去。

##### 难点和解决方法

1. 我个人认为这个项目的难点主要就在于同步的机制，这也是整个系统之所以能体现出分布式特点的地方。就是比如说一个用户连接上某一个文件服务器之后，对它进行了一个操作，然后我需要把这个操作传递到所有的文件服务器上。
2. 还有一个是文件服务器的路径问题。我这个文件服务器是模仿linux去开发的，但我本质上还是在windows环境去去编写的代码，而且也没有做到分布式，其实都是在本机上面跑多个程序来模拟的，因此路径设计虽然说不是一个很难的点，但当时确实做了一些思考。我是在本机下开了一个data的文件夹，然后里面细分出了每个文件服务器一个文件夹，就类似虚拟机划分磁盘这样。每个文件夹就相当于文件服务器的根目录，就是linux的～。一开始我用的是/data/fileserver1/...，后面发现这样的话系统灵活性不够，应该是指定根目录，然后文件系统内都用相对路径，这样在锁服务器的时候对某一个文件上锁，也就可以对所有副本服务器的对应文件上锁了。
3. 还有一个是对于用户的访问控制，我现在是每个用户都公用文件服务器的所有内容，如果想要每个用户可以有自己专属的文件的话，就需要加上访问控制，要使用访问控制的话首先需要加一个类似session的机制，用户每次把自己的用户名传过来服务器，服务器就知道是哪一个用户的操作请求，让后去ACL表中迭代解析看看他是否具有访问目标文件或者路径的权限，如果没有的话就返回deny。





#### 微信小程序

##### 登陆会话 安全性

1. 目前使用的是微信服务提供的checksession接口判断是否登陆

2. 后面准备加入一个token机制，客户端只保存sessionid替代掉现在用的openid，然后session保存在后台的redis中，这个使用flask-session的方法就可以实现，将session保存在本地。
3. flask内置的session是把session全部保存在客户端的cookie中，对隐私数据保存存在风险。



##### 正则表达式

| 元字符 | 描述                                                         |
| :----: | ------------------------------------------------------------ |
|   .    | 匹配除换行符以外的任意字符。                                 |
|  [ ]   | 字符类，匹配方括号中包含的任意字符。                         |
|  [^ ]  | 否定字符类。匹配方括号中不包含的任意字符                     |
|   *    | 匹配前面的子表达式零次或多次                                 |
|   +    | 匹配前面的子表达式一次或多次                                 |
|   ?    | 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。       |
| {n,m}  | 花括号，匹配前面字符至少 n 次，但是不超过 m 次。             |
| (xyz)  | 字符组，按照确切的顺序匹配字符xyz。                          |
| &#124; | 分支结构，匹配符号之前的字符或后面的字符。                   |
| &#92;  | 转义符，它可以还原元字符原来的含义，允许你匹配保留字符 `[ ] ( ) { } . * + ? ^  \ ` |
|   ^    | 匹配行的开始                                                 |
|   $    | 匹配行的结束                                                 |

##### 难点和解决方法

其实微信小程序的开发过程呢遇到的技术难点不多，因为小程序成功与否主要还是在于前端的设计开发上决定的，后端很多时候只是充当一个前端和数据库交互的一个中间件这样。不过在开发工程中还是会有很多思考的地方。

1. 比如说一个电商的小程序，安全性会很重要。之前的话会话都是交给微信去管理，就是用户调用微信的登陆接口后，会和微信那边的后台建立一个session，然后后台会告诉前台用户在我们数据库的ID，然后用这个id来通信，一旦涉及到id传递的时候，就会先去微信的后台checksession检验会话是否还连接。

   不过因为考虑到电商涉及到比较大的金钱交易，我们更希望将会话能控制在自己手里。所以呢我就又去研究了一下flask的session机制，就用两层的会话来保证安全性。flask的session是吧session信息完全保存在客户端的cookie中，这样安全性还是有一些担忧的，可能会泄漏用户的信息，所以后面就采用了flask-session的组件，他可以通过编写一个配置文件去把数据存储在本地的数据库中，客户端就只拿一个sessionid，这样就可以防止用户信息泄漏了。

   

2. 选一个稍微会复杂一些的模块：优惠券模块来讲。我们现在这边需要给电商实现一个优惠券的逻辑，用户在消费的时候可以选取可用的优惠券进行消费。我一开始的想法是把优惠券分成很多不同的种类，每个种类对应一套逻辑。不过在实践中发现，优惠券的种类十分繁多，就包括直接抵扣10块，或者说是需要一定条件的，满多少减多少，还有什么品牌才能减这样。这样子的话就需要实现很多套逻辑，代码也比较复杂。所以后面思考后，就将一些功能比较类似的优惠券合并了，我数据库中定义一张优惠券的表，每个一种类型的优惠券都会有很多的属性，比如说他减多少钱啊，满多少才减，适用范围啊这样。这样有一些优惠券就可以合并了，比如刚刚说的直接抵扣10元，其实也就是相当于满0元减10元，这样的话编程难度也降低了，而且思路会更清晰。





#### Flask后端项目

##### Flask框架基础

看onenote笔记



##### ORM框架优缺点

优点：

1. 将数据库的记录转换成熟悉的类对象操作，不用书写复杂的sql语句，将算法逻辑和数据存储的逻辑分离，使得代码更清晰明了。
2. 提高开发效率，提高了开发效率。由于ORM可以自动对Entity对象与数据库中的Table进行字段与属性的映射，所以我们实际可能已经不需要一个专用的、庞大的数据访问层。 

缺点：
1）自动化进行关系数据库的映射需要**消耗系统性能**。
2）在**处理多表联查**、**where条件复杂之类**的查询时，ORM的语法会变得复杂。



#### 百度爬虫项目

##### scrapy框架之spider

1. 这个框架是一个爬虫的框架，在启动爬虫后会调用start_request函数，所以在里面对百度的url进行初始的请求，wd决定搜索的关键词，pn决定第几页，rn决定每一页的链接数量。使用scrapy.request去请求，meta可以传递参数到response中，callback决定回调函数parse。
2. 在parse中对请求的结果进行处理，首先在scrapy shell中调试xpath如何去获取想要的数据，然后在代码中编写xpath规则，并将获得数据处理并放入item中。使用yield去返回item。其实是相当于生成一个生成器。同时也可以用request进一步请求下一页的链接，这个可以决定搜索的深度。
3. 使用xpath解析出下一页的链接，然后可以进行翻页，也可以用rn翻页，但因为百度搜索结果每一次不同，一般是76页，有时候没有，容易发生数据重复。



##### body_path

在百度新闻搜索结果中，有各种各样新闻媒体的帖子或者新闻，比如百度自己的百家号这样的，我们需要进入这些链接进一步去爬取body_path的内容，但这个正文部分的标签不好统一，因此我查看了一些比较出名的新闻网站，然后记录下他们正文部分的标签名称，然后写入到一个列表中。

在爬某个新闻页面的时候，就遍历这个列表去查看是否有有效的标签，如果有的话就认为是正文爬取下来。



##### customUserAgent

scrapy爬虫本质上还是在伪装成浏览器去爬取网页，user_agent用来表示用户当前使用的浏览器版本，操作系统等，因此还需要在请求头部加上这个信息。在中间件中加上一个类（类中要继承useragentMiddleware类，并实现processs_request函数，参数为request），在请求的头部加上user_agent字段。注意不能混入手机浏览器的头部，因为使用手机浏览器头部会请求到手机端看到的页面，这个页面和电脑版的xpath路径不同，爬不到数据。



##### pipeline

因为项目有两种spider，分别是百度搜索和百度爬虫，他们的item不一样。因此使用isinstance判断是哪一种的item，进行不同的处理，存入csv文件中。



##### robot协议

robots.txt 是遵循 Robot协议 的一个文件，它保存在网站的服务器中，它的作用是，告诉搜索引擎爬虫，本网站哪些目录下的网页 不希望 你进行爬取收录。**在Scrapy启动后，会在第一时间访问网站的 robots.txt 文件，然后决定该网站的爬取范围。**



##### 运行scrapy

scrapy crawl searchspider (爬虫名称)



##### 难点和解决方法

1. 验证码问题 301 302时休息一下重新请求
2. 爬虫翻页问题，一开始用rn翻页，然后发现搜索结果的页面数量不一定都是76，就改成解析，下一页按钮的链接来翻页
3. 大坑，某些关键词爬取数据丢失，因为使用了手机的user-agent
4. 百度新闻正文部分，查了一些比较著名企业的正文标签然后爬取。





### 聊天

1. 为什么选择字节跳动？
   - 字节的offer保留时间长，可以自己挑时间，适合我们这种课比较多专业的学生
   - 作为上升期的企业，工作量更大，更能锻 炼人学到知识
   - 字节一年四季招人，对各种人才的需求比较大，希望我能对字节跳动的发展提供一些帮助。
   - 字节的薪资比一般的大厂高
   
2. 你的优势是什么？
   - 我认为一个主要的点是我有一些团队开发的经验，包括作为团队的成员，或者团队的负责人，应该怎么样更好的完成本职工作。这些其实都是自己慢慢摸索出来的，也不想正式公司企业那样会有严格的规定。
   - 我的基础知识还是比较牢固的，而且学习能力和态度都比较好，愿意去学习新的知识。就像我除了学校课程外，也会去参加一些项目或者是学习竞赛。
   - 我有一些web端的后台开发经验，对一些后台开发的框架和组件有一些了解，虽然可能不是公司使用的C++语言的框架，但我想可能思想还是通用的。
   - 因为要部署web 服务器，所以接触过linux系统和指令，对linux有一定的了解。
   
3. 为什么选择腾讯而不选择字节跳动？

   - 我作为一个学生在找实习工作的时候其实对公司和部门并不是特别的了解，因为我没有亲生在环境中工作过，所以最直接的就是在和面试官交流的时候留下的感受。腾讯的面试官更有亲和力一些，在交流的时候也会积极回应，但字节的面试官，位置会放的比较高，不怎么理会我，给我造成很大压力，所以可能也可以一定程度上看出部门的氛围如何，我认为腾讯的部门氛围可能更好一些。
   - 第二是从情怀来看，腾讯的产品也是陪着我们这一辈长大的，不管是社交平台还是各种游戏，都是玩腾讯的，q币都冲了不少，所以对这个公司一直有很重的情怀，现在有机会能加入成为一员肯定会来的。
   - 第三是公司底蕴，虽然有的公司给的实习工资会比腾讯还高出很多，但往往还是比较难吸引到人才，因为腾讯这个公司无论规模还是底蕴都是绝大多数互联网企业比不过的，如果能在这里工作肯定是大家都向往的。

4. 你对PCG了解多少：

   在第一轮面试的时候我也交流了，就是我们部门主要还是将看点的一些推荐算法等部署上线，不知道理解的正不正确。

   平台与内容的事业群，旗下有腾讯新闻、腾讯看点、微视这些模块。个人认为这个部门的核心就体现在名字中平台和内容两个部分。首先就是说要去提供一个平台，向微视这样的小视频平台，或者说是看点、腾讯新闻这样的新闻媒体平台用户服务，同时借助这个平台让通过用户去创建内容，比如小视频，帖子这种。另外一个核心就在于如何将内容用机器学习的推荐算法等去更好的在平台上展示给用户。然后这些算法编写出来，部署上线也是非常重要的一环，所以我们的工作核心可能就这里。

5. 为什么不考研

   - 考研很辛苦，浪费很多时间。应试教育，学到的东西实用性不强
   - 考研有风险，往上考难度大，失败了就浪费了时间

6. 为什么不保研

   - 中大整体包括我们本科的计算机水平还是不错的，但是毕竟是综合类院校，确实计算机方向的研究生教育的水平稍微弱了一些，个人认为在本校保研还要看选中的导师如何，所以读出来不一定会给我的履历增添很多光彩。
   - 要往外校考的话，本身难度会比较大，因为外校一般更喜欢本校的学生。其次中大的位置还是比较尴尬的，再上一档可能就会想去到南京大学、浙江大学这样的档次，难度也是相当大。

7. 要问什么问题吗？
   - 可以进一步介绍一下部门和工作吗？
   - 实习生是如何培养的
   - 关于转正
   - 想问的大概就这些，也不耽误您时间了
